# -*- coding: utf-8 -*-
"""TEXT SUMMARIZE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19UYyDVDjNihCYX-nrGfSIHq4Msjucy0p
"""

!pip install openai gradio transformers sentencepiece --quiet
from openai import OpenAI
import gradio as gr
from transformers import pipeline

client = OpenAI(api_key="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")# Replace with your actual key")  # Replace with your actual API key

# Hugging Face fallback model
hf_summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# for uploding the file
def read_file(file_obj):
    """Read uploaded file content safely."""
    if file_obj is None:
        return ""
    try:
        # Gradio now passes a temporary file path, not raw bytes
        with open(file_obj.name, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        return f"Error reading file: {e}"
def summarize(text, style):
    if not text.strip():
        return "Error: provide text or file."

    # Summarization prompt
    prompts = {
        "Brief": f"Summarize briefly:\n\n{text}",
        "Detailed": f"Summarize in detail:\n\n{text}",
        "Bullet Points": f"Summarize as bullet points:\n\n{text}",
    }
    prompt = prompts.get(style, f"Summarize the following text:\n\n{text}")
    ## for handling the error
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful summarization assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        summary = response.choices[0].message.content

    except Exception as e:
        print("⚠️ OpenAI API Error:", e)
        if "insufficient_quota" in str(e) or "429" in str(e):
            print("➡️ Switching to Hugging Face fallback model...")
            try:
                hf_summary = hf_summarizer(text, max_length=180, min_length=40, do_sample=False)
                summary = hf_summary[0]['summary_text']
            except Exception as hf_error:
                summary = f"Error using fallback model: {hf_error}"
        else:
            summary = "Error: " + str(e)

    return summary

# Gradio Interface
iface = gr.Interface(
    fn=lambda text, file, style: summarize(text or read_file(file), style),
    inputs=[
        gr.Textbox(lines=20, placeholder="Paste your text...", label="Text Input"),
        gr.File(label="Or upload a text file"),
        gr.Dropdown(["Brief", "Detailed", "Bullet Points"], value="Brief", label="Summarization Style")
    ],
    outputs=gr.Textbox(label="Summary"),
    title="Smart Document Summarizer",
    description="Automatically uses GPT-3.5 if available; switches to Hugging Face if OpenAI quota runs out."
)
iface.launch()

